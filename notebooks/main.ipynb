{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "451031bb",
   "metadata": {},
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d52726c",
   "metadata": {},
   "source": [
    "### Data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b0d133",
   "metadata": {},
   "source": [
    "##### Required Python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e50d6f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing duckdb...\n",
      "Collecting duckdb\n",
      "  Downloading duckdb-1.3.2-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (7.0 kB)\n",
      "Downloading duckdb-1.3.2-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: duckdb\n",
      "Successfully installed duckdb-1.3.2\n",
      "pandas already installed\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# We list special packages that don't exist in jupyter installation\n",
    "special_required_packages = {\n",
    "    \"duckdb\": \"duckdb\",\n",
    "    \"pandas\": \"pandas\"\n",
    "}\n",
    "\n",
    "# Verify special packages\n",
    "for module_name, pip_name in special_required_packages.items():\n",
    "    try:\n",
    "        importlib.import_module(module_name)\n",
    "        print(f\"{module_name} already installed\")\n",
    "    except ImportError:\n",
    "        print(f\"Installing {pip_name}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pip_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ed9d0ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import duckdb\n",
    "import shutil\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a80d19",
   "metadata": {},
   "source": [
    "##### Verify system version and set input data path, database path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c75ec46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: /home/jovyan/challenge\n",
      "Python version: 3.11.6 | packaged by conda-forge | (main, Oct  3 2023, 10:40:35) [GCC 12.3.0]\n"
     ]
    }
   ],
   "source": [
    "# Input data path and database path within container\n",
    "CSV_PATH = \"/home/jovyan/challenge/data/ads_spend.csv\"\n",
    "DB_PATH = \"/home/jovyan/challenge/database/warehouse.db\"\n",
    "\n",
    "# Verify directory path and python version\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "print(f\"Python version: {sys.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e28cbd9",
   "metadata": {},
   "source": [
    "##### Explore first 5 rows within input data (ads_spend.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40a36408",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "date",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "platform",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "account",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "campaign",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "country",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "device",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "spend",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "clicks",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "impressions",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "conversions",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "bd0d5889-90ba-426b-b68d-5ad87a65373d",
       "rows": [
        [
         "0",
         "2025-01-01",
         "Meta",
         "AcctA",
         "Prospecting",
         "MX",
         "Desktop",
         "1115.94",
         "360",
         "15840",
         "29"
        ],
        [
         "1",
         "2025-01-01",
         "Google",
         "AcctA",
         "Brand_Search",
         "CA",
         "Mobile",
         "789.43",
         "566",
         "22640",
         "28"
        ],
        [
         "2",
         "2025-01-01",
         "Google",
         "AcctA",
         "Prospecting",
         "BR",
         "Desktop",
         "381.4",
         "133",
         "10241",
         "12"
        ],
        [
         "3",
         "2025-01-01",
         "Google",
         "AcctC",
         "Prospecting",
         "US",
         "Desktop",
         "1268.34",
         "891",
         "49005",
         "36"
        ],
        [
         "4",
         "2025-01-01",
         "Google",
         "AcctA",
         "Brand_Search",
         "BR",
         "Desktop",
         "1229.7",
         "628",
         "21352",
         "31"
        ]
       ],
       "shape": {
        "columns": 10,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>platform</th>\n",
       "      <th>account</th>\n",
       "      <th>campaign</th>\n",
       "      <th>country</th>\n",
       "      <th>device</th>\n",
       "      <th>spend</th>\n",
       "      <th>clicks</th>\n",
       "      <th>impressions</th>\n",
       "      <th>conversions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-01-01</td>\n",
       "      <td>Meta</td>\n",
       "      <td>AcctA</td>\n",
       "      <td>Prospecting</td>\n",
       "      <td>MX</td>\n",
       "      <td>Desktop</td>\n",
       "      <td>1115.94</td>\n",
       "      <td>360</td>\n",
       "      <td>15840</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-01-01</td>\n",
       "      <td>Google</td>\n",
       "      <td>AcctA</td>\n",
       "      <td>Brand_Search</td>\n",
       "      <td>CA</td>\n",
       "      <td>Mobile</td>\n",
       "      <td>789.43</td>\n",
       "      <td>566</td>\n",
       "      <td>22640</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-01-01</td>\n",
       "      <td>Google</td>\n",
       "      <td>AcctA</td>\n",
       "      <td>Prospecting</td>\n",
       "      <td>BR</td>\n",
       "      <td>Desktop</td>\n",
       "      <td>381.40</td>\n",
       "      <td>133</td>\n",
       "      <td>10241</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-01-01</td>\n",
       "      <td>Google</td>\n",
       "      <td>AcctC</td>\n",
       "      <td>Prospecting</td>\n",
       "      <td>US</td>\n",
       "      <td>Desktop</td>\n",
       "      <td>1268.34</td>\n",
       "      <td>891</td>\n",
       "      <td>49005</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-01-01</td>\n",
       "      <td>Google</td>\n",
       "      <td>AcctA</td>\n",
       "      <td>Brand_Search</td>\n",
       "      <td>BR</td>\n",
       "      <td>Desktop</td>\n",
       "      <td>1229.70</td>\n",
       "      <td>628</td>\n",
       "      <td>21352</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date platform account      campaign country   device    spend  \\\n",
       "0  2025-01-01     Meta   AcctA   Prospecting      MX  Desktop  1115.94   \n",
       "1  2025-01-01   Google   AcctA  Brand_Search      CA   Mobile   789.43   \n",
       "2  2025-01-01   Google   AcctA   Prospecting      BR  Desktop   381.40   \n",
       "3  2025-01-01   Google   AcctC   Prospecting      US  Desktop  1268.34   \n",
       "4  2025-01-01   Google   AcctA  Brand_Search      BR  Desktop  1229.70   \n",
       "\n",
       "   clicks  impressions  conversions  \n",
       "0     360        15840           29  \n",
       "1     566        22640           28  \n",
       "2     133        10241           12  \n",
       "3     891        49005           36  \n",
       "4     628        21352           31  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Verify if csv exists\n",
    "if os.path.exists(CSV_PATH):\n",
    "    \n",
    "    # Take first 5 rows into pandas format, just for inspection\n",
    "    df_sample = pd.read_csv(CSV_PATH, nrows=5)\n",
    "    display(df_sample)\n",
    "    \n",
    "else:\n",
    "    print(\"CSV file not found\")\n",
    "\n",
    "# Delete dataframe\n",
    "del df_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9619075",
   "metadata": {},
   "source": [
    "##### Some features and big picture within input data (ads_spend.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f9bfb68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (2000, 10)\n",
      "Columns: ['date', 'platform', 'account', 'campaign', 'country', 'device', 'spend', 'clicks', 'impressions', 'conversions']\n",
      "Total spent: $1,690,764.32\n",
      "Date range: 2025-01-01 a 2025-06-30\n",
      "Platforms: ['Meta' 'Google']\n",
      "Unique accounts: ['AcctA' 'AcctC' 'AcctB']\n",
      "  date: object\n",
      "  platform: object\n",
      "  account: object\n",
      "  campaign: object\n",
      "  country: object\n",
      "  device: object\n",
      "  spend: float64\n",
      "  clicks: int64\n",
      "  impressions: int64\n",
      "  conversions: int64\n"
     ]
    }
   ],
   "source": [
    "# Verify if csv exists\n",
    "if os.path.exists(CSV_PATH):\n",
    "    \n",
    "    # Full data set info\n",
    "    df_full = pd.read_csv(CSV_PATH)\n",
    "    print(f\"Dataset shape: {df_full.shape}\")\n",
    "    print(f\"Columns: {list(df_full.columns)}\")\n",
    "    print(f\"Total spent: ${df_full['spend'].sum():,.2f}\")\n",
    "    print(f\"Date range: {df_full['date'].min()} a {df_full['date'].max()}\")\n",
    "    print(f\"Platforms: {df_full['platform'].unique()}\")\n",
    "    print(f\"Unique accounts: {df_full['account'].unique()}\")\n",
    "    \n",
    "    # Verify data types\n",
    "    for col, dtype in df_full.dtypes.items():\n",
    "        print(f\"  {col}: {dtype}\")\n",
    "        \n",
    "else:\n",
    "    print(\"CSV file not found\")\n",
    "\n",
    "# Delete dataframe\n",
    "del df_full"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858f976b",
   "metadata": {},
   "source": [
    "### Data ingestion process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cc8db4",
   "metadata": {},
   "source": [
    "##### Function to ingest data in database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51fde355",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_data():\n",
    "\n",
    "    # Error handling\n",
    "    try:\n",
    "\n",
    "        # Verify csv file again\n",
    "        if not os.path.exists(CSV_PATH):\n",
    "            raise FileNotFoundError(f\"{CSV_PATH} csv not found\")\n",
    "            \n",
    "        # Read CSV and convert to dataframe\n",
    "        input_data_df = pd.read_csv(CSV_PATH)\n",
    "        print(f\"csv loaded: {input_data_df.shape[0]:,} rows, {input_data_df.shape[1]} columns\")\n",
    "        \n",
    "        # Validate if csv is empty\n",
    "        if input_data_df.empty:\n",
    "            raise ValueError(\"csv is empty\")\n",
    "\n",
    "        # Validate columns name (we put the list of columns that we already know exist)    \n",
    "        required_columns = ['date', 'platform', 'account', 'campaign', \n",
    "                           'country', 'device', 'spend', 'clicks', \n",
    "                           'impressions', 'conversions']\n",
    "        \n",
    "        # Check if we have missing columns\n",
    "        missing_cols = [col for col in required_columns if col not in input_data_df.columns]\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"Missing columns: {missing_cols}\")\n",
    "\n",
    "        # Check null or empty values in columns\n",
    "        null_empty_columns = [col for col in required_columns if input_data_df[col].isnull().any() or input_data_df[col].apply(lambda x: isinstance(x, str) and x.strip() == \"\").any()]\n",
    "\n",
    "        if null_empty_columns:\n",
    "            raise ValueError(f\"Columnas con valores nulos o vacíos: {null_empty_columns}\")\n",
    "        \n",
    "        # Add metadata\n",
    "        # Date\n",
    "        input_data_df['load_date'] = datetime.now()\n",
    "        # Filename\n",
    "        input_data_df['source_file_name'] = 'ads_spend.csv'\n",
    "        \n",
    "        print(f\"load_date: {input_data_df['load_date'].iloc[0]}\")\n",
    "        print(f\"source_file_name: {input_data_df['source_file_name'].iloc[0]}\")\n",
    "        \n",
    "        # Connect to duckDB\n",
    "        conn = duckdb.connect(DB_PATH)\n",
    "        \n",
    "        create_table_sql = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS ads_spend_db (\n",
    "            date DATE,\n",
    "            platform VARCHAR,\n",
    "            account VARCHAR,\n",
    "            campaign VARCHAR,\n",
    "            country VARCHAR,\n",
    "            device VARCHAR,\n",
    "            spend DECIMAL(12,2),\n",
    "            clicks INTEGER,\n",
    "            impressions INTEGER,\n",
    "            conversions INTEGER,\n",
    "            -- Metadata challenge required\n",
    "            load_date TIMESTAMP,\n",
    "            source_file_name VARCHAR\n",
    "        );\n",
    "        \"\"\"\n",
    "        # Verify table is already created or just verify\n",
    "        conn.execute(create_table_sql)\n",
    "        print(\"Table ads_spend_db verified/created\")\n",
    "        \n",
    "        # Count register before add\n",
    "        count_before = conn.execute(\"SELECT COUNT(*) FROM ads_spend_db\").fetchone()[0]\n",
    "        \n",
    "        # Insert data in append mode to demostrate persistence\n",
    "        conn.register('df_new', input_data_df)\n",
    "        conn.execute(\"INSERT INTO ads_spend_db SELECT * FROM df_new\")\n",
    "        \n",
    "        # Count register after add\n",
    "        count_after = conn.execute(\"SELECT COUNT(*) FROM ads_spend_db\").fetchone()[0]\n",
    "        \n",
    "        conn.close()\n",
    "        \n",
    "        # Result for n8n in JSON format\n",
    "        result = {\n",
    "            \"status\": \"success\",\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"rows_inserted\": len(input_data_df),\n",
    "            \"total_rows_before\": count_before,\n",
    "            \"total_rows_after\": count_after,\n",
    "            \"source_file\": \"ads_spend.csv\",\n",
    "            \"message\": f\"Successfully ingested {len(input_data_df):,} rows into warehouse\"\n",
    "        }\n",
    "        \n",
    "        print(f\"Register added: {len(input_data_df):,}\")\n",
    "        print(f\"Total regiser in DB now: {count_after:,}\")\n",
    "        print(f\"Increment: +{count_after - count_before:,}\")\n",
    "        \n",
    "        return result\n",
    "\n",
    "    # Error handling    \n",
    "    except Exception as error:\n",
    "        error_result = {\n",
    "            \"status\": \"error\",\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"error_message\": str(error),\n",
    "            \"error_type\": type(error).__name__\n",
    "        }\n",
    "        print(f\"ERROR: {error}\")\n",
    "        return error_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340f2bd9",
   "metadata": {},
   "source": [
    "##### Execute data ingest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d559296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Ingest in Execution\n",
      "csv loaded: 2,000 rows, 10 columns\n",
      "load_date: 2025-08-28 04:59:58.325771\n",
      "source_file_name: ads_spend.csv\n",
      "Table ads_spend_db verified/created\n",
      "Register added: 2,000\n",
      "Total regiser in DB now: 16,000\n",
      "Increment: +2,000\n",
      "\n",
      "Final Result: \n",
      "{\n",
      "  \"status\": \"success\",\n",
      "  \"timestamp\": \"2025-08-28T04:59:58.669323\",\n",
      "  \"rows_inserted\": 2000,\n",
      "  \"total_rows_before\": 14000,\n",
      "  \"total_rows_after\": 16000,\n",
      "  \"source_file\": \"ads_spend.csv\",\n",
      "  \"message\": \"Successfully ingested 2,000 rows into warehouse\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Ingest data to database\n",
    "print(\"Data Ingest in Execution\")\n",
    "result = ingest_data()\n",
    "\n",
    "# Get result for n8n\n",
    "print(f\"\\nFinal Result: \")\n",
    "print(json.dumps(result, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766e52d9",
   "metadata": {},
   "source": [
    "### Verify data persistence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29c678f",
   "metadata": {},
   "source": [
    "##### Function to verify persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d21a9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_persistence():\n",
    "\n",
    "    # Connect to database    \n",
    "    conn = duckdb.connect(DB_PATH)\n",
    "    \n",
    "    # Get general statistics\n",
    "    total_records = conn.execute(\"SELECT COUNT(*) FROM ads_spend_db\").fetchone()[0]\n",
    "    \n",
    "    # Get load info\n",
    "    load_info = conn.execute(\"\"\"\n",
    "        SELECT \n",
    "            source_file_name,\n",
    "            strftime(load_date,'%Y-%m-%d %H:%M') as load_minute,\n",
    "            COUNT(*) as record_count,\n",
    "            MIN(load_date) as first_load_time,\n",
    "            MAX(load_date) as last_load_time\n",
    "        FROM ads_spend_db \n",
    "        GROUP BY source_file_name, load_minute\n",
    "        ORDER BY last_load_time DESC\n",
    "    \"\"\").fetchall()\n",
    "    \n",
    "    print(f\"Total records persisted {total_records:,}\")\n",
    "    print(f\"Registered load sessions {len(load_info)}\")\n",
    "    \n",
    "    print(f\"\\nLoad history\")\n",
    "    # Get last 5 five loads\n",
    "    for load in load_info[:5]:\n",
    "        filename, load_date, count, first_time, last_time = load\n",
    "        print(f\"{filename} | {load_date}\")\n",
    "        print(f\"Registers: {count:,}\")\n",
    "        print(f\"Datetime: {first_time}\")\n",
    "        print()\n",
    "    \n",
    "    # Verify data quality\n",
    "    data_quality = conn.execute(\"\"\"\n",
    "        SELECT \n",
    "            COUNT(*) as total_records,\n",
    "            COUNT(DISTINCT date) as unique_dates,\n",
    "            COUNT(DISTINCT platform) as unique_platforms,\n",
    "            COUNT(DISTINCT account) as unique_accounts,\n",
    "            SUM(spend) as total_spend,\n",
    "            SUM(conversions) as total_conversions,\n",
    "            AVG(spend) as avg_spend_per_record\n",
    "        FROM ads_spend_db\n",
    "    \"\"\").fetchone()\n",
    "    \n",
    "    print(f\"Data queality:\")\n",
    "    print(f\"Total registers: {data_quality[0]:,}\")\n",
    "    print(f\"Unique dates: {data_quality[1]:,}\")  \n",
    "    print(f\"Unique platforms: {data_quality[2]}\")\n",
    "    print(f\"Unique accounts: {data_quality[3]:,}\")\n",
    "    print(f\"Total spent: ${data_quality[4]:,.2f}\")\n",
    "    print(f\"Total conversions: {data_quality[5]:,}\")\n",
    "    print(f\"Average spent per register: ${data_quality[6]:.2f}\")\n",
    "    \n",
    "    # Get last data\n",
    "    sample_data = conn.execute(\"\"\"\n",
    "        SELECT date, platform, account, spend, conversions, load_date\n",
    "        FROM ads_spend_db \n",
    "        ORDER BY load_date DESC \n",
    "        LIMIT 5\n",
    "    \"\"\").fetchall()\n",
    "    \n",
    "    print(f\"\\nLast data:\")\n",
    "    for row in sample_data:\n",
    "        date, platform, account, spend, conversions, load_date = row\n",
    "        print(f\"   {date} | {platform} | {account} | ${spend} | {conversions} conv | {load_date}\")\n",
    "    \n",
    "    conn.close()\n",
    "    \n",
    "    return {\n",
    "        \"status\": \"verified\", \n",
    "        \"total_records\": total_records, \n",
    "        \"load_sessions\": len(load_info),\n",
    "        \"data_quality_check\": \"passed\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5abd52",
   "metadata": {},
   "source": [
    "##### Execute persistence verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d269da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records persisted 16,000\n",
      "Registered load sessions 8\n",
      "\n",
      "Load history\n",
      "ads_spend.csv | 2025-08-28 04:59\n",
      "Registers: 2,000\n",
      "Datetime: 2025-08-28 04:59:58.325771\n",
      "\n",
      "ads_spend.csv | 2025-08-28 04:24\n",
      "Registers: 2,000\n",
      "Datetime: 2025-08-28 04:24:44.721864\n",
      "\n",
      "ads_spend.csv | 2025-08-28 04:23\n",
      "Registers: 2,000\n",
      "Datetime: 2025-08-28 04:23:47.079539\n",
      "\n",
      "ads_spend.csv | 2025-08-28 01:59\n",
      "Registers: 2,000\n",
      "Datetime: 2025-08-28 01:59:55.243061\n",
      "\n",
      "ads_spend.csv | 2025-08-28 01:56\n",
      "Registers: 2,000\n",
      "Datetime: 2025-08-28 01:56:01.048397\n",
      "\n",
      "Data queality:\n",
      "Total registers: 16,000\n",
      "Unique dates: 181\n",
      "Unique platforms: 2\n",
      "Unique accounts: 3\n",
      "Total spent: $13,526,114.56\n",
      "Total conversions: 439,336\n",
      "Average spent per register: $845.38\n",
      "\n",
      "Last data:\n",
      "   2025-01-01 | Meta | AcctA | $1115.94 | 29 conv | 2025-08-28 04:59:58.325771\n",
      "   2025-01-01 | Google | AcctA | $789.43 | 28 conv | 2025-08-28 04:59:58.325771\n",
      "   2025-01-01 | Google | AcctA | $381.40 | 12 conv | 2025-08-28 04:59:58.325771\n",
      "   2025-01-01 | Google | AcctC | $1268.34 | 36 conv | 2025-08-28 04:59:58.325771\n",
      "   2025-01-01 | Google | AcctA | $1229.70 | 31 conv | 2025-08-28 04:59:58.325771\n",
      "\n",
      "Verification result: {'status': 'verified', 'total_records': 16000, 'load_sessions': 8, 'data_quality_check': 'passed'}\n"
     ]
    }
   ],
   "source": [
    "# Execute verification\n",
    "persistence_result = data_persistence()\n",
    "print(f\"\\nVerification result: {persistence_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca24934a",
   "metadata": {},
   "source": [
    "### Create n8n executable script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f781e5",
   "metadata": {},
   "source": [
    "##### Create script for n8n in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "07683cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python script content\n",
    "script_content_ingest_data = '''#!/usr/bin/env python3\n",
    "\n",
    "import importlib\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# We list special packages that don't exist in jupyter installation\n",
    "special_required_packages = {\n",
    "    \"duckdb\": \"duckdb\",\n",
    "    \"pandas\": \"pandas\"\n",
    "}\n",
    "\n",
    "# Verify special packages\n",
    "for module_name, pip_name in special_required_packages.items():\n",
    "    try:\n",
    "        importlib.import_module(module_name)\n",
    "        print(f\"{module_name} already installed\")\n",
    "    except ImportError:\n",
    "        print(f\"Installing {pip_name}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pip_name])\n",
    "\n",
    "#import sys\n",
    "import pandas as pd\n",
    "import duckdb\n",
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "\n",
    "        # Localprobe\n",
    "        #csv_path = \"/home/jovyan/challenge/data/ads_spend.csv\"\n",
    "        #db_path = \"/home/jovyan/challenge/database/warehouse.db\"\n",
    "        # Production\n",
    "        csv_path = \"/data/ads_spend.csv\"\n",
    "        db_path = \"/database/warehouse.db\"\n",
    "\n",
    "        # Verify csv file again\n",
    "        if not os.path.exists(csv_path):\n",
    "            raise FileNotFoundError(f\"{csv_path} csv not found\")\n",
    "            \n",
    "        # Read CSV and convert to dataframe\n",
    "        input_data_df = pd.read_csv(csv_path)\n",
    "        print(f\"csv loaded: {input_data_df.shape[0]:,} rows, {input_data_df.shape[1]} columns\")\n",
    "        \n",
    "        # Validate if csv is empty\n",
    "        if input_data_df.empty:\n",
    "            raise ValueError(\"csv is empty\")\n",
    "\n",
    "        # Validate columns name (we put the list of columns that we already know exist)    \n",
    "        required_columns = ['date', 'platform', 'account', 'campaign', \n",
    "                           'country', 'device', 'spend', 'clicks', \n",
    "                           'impressions', 'conversions']\n",
    "        \n",
    "        # Check if we have missing columns\n",
    "        missing_cols = [col for col in required_columns if col not in input_data_df.columns]\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"Missing columns: {missing_cols}\")\n",
    "\n",
    "        # Check null or empty values in columns\n",
    "        null_empty_columns = [col for col in required_columns if input_data_df[col].isnull().any() or input_data_df[col].apply(lambda x: isinstance(x, str) and x.strip() == \"\").any()]\n",
    "\n",
    "        if null_empty_columns:\n",
    "            raise ValueError(f\"Columnas con valores nulos o vacíos: {null_empty_columns}\")\n",
    "        \n",
    "        # Add metadata\n",
    "        # Date\n",
    "        input_data_df['load_date'] = datetime.now()\n",
    "        # Filename\n",
    "        input_data_df['source_file_name'] = 'ads_spend.csv'\n",
    "        \n",
    "        print(f\"load_date: {input_data_df['load_date'].iloc[0]}\")\n",
    "        print(f\"source_file_name: {input_data_df['source_file_name'].iloc[0]}\")\n",
    "        \n",
    "        # Connect to duckDB\n",
    "        conn = duckdb.connect(db_path)\n",
    "        \n",
    "        create_table_sql = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS ads_spend_db (\n",
    "            date DATE,\n",
    "            platform VARCHAR,\n",
    "            account VARCHAR,\n",
    "            campaign VARCHAR,\n",
    "            country VARCHAR,\n",
    "            device VARCHAR,\n",
    "            spend DECIMAL(12,2),\n",
    "            clicks INTEGER,\n",
    "            impressions INTEGER,\n",
    "            conversions INTEGER,\n",
    "            -- Metadata challenge required\n",
    "            load_date TIMESTAMP,\n",
    "            source_file_name VARCHAR\n",
    "        );\n",
    "        \"\"\"\n",
    "        # Verify table is already created or just verify\n",
    "        conn.execute(create_table_sql)\n",
    "        print(\"Table ads_spend_db verified/created\")\n",
    "        \n",
    "        # Count register before add\n",
    "        count_before = conn.execute(\"SELECT COUNT(*) FROM ads_spend_db\").fetchone()[0]\n",
    "        \n",
    "        # Insert data in append mode to demostrate persistence\n",
    "        conn.register('df_new', input_data_df)\n",
    "        conn.execute(\"INSERT INTO ads_spend_db SELECT * FROM df_new\")\n",
    "        \n",
    "        # Count register after add\n",
    "        count_after = conn.execute(\"SELECT COUNT(*) FROM ads_spend_db\").fetchone()[0]\n",
    "        \n",
    "        conn.close()\n",
    "        \n",
    "        # Result for n8n in JSON format\n",
    "        result = {\n",
    "            \"status\": \"success\",\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"rows_inserted\": len(input_data_df),\n",
    "            \"total_rows_before\": count_before,\n",
    "            \"total_rows_after\": count_after,\n",
    "            \"source_file\": \"ads_spend.csv\",\n",
    "            \"message\": f\"Successfully ingested {len(input_data_df):,} rows into warehouse\"\n",
    "        }\n",
    "        \n",
    "        print(f\"Register added: {len(input_data_df):,}\")\n",
    "        print(f\"Total regiser in DB now: {count_after:,}\")\n",
    "        print(f\"Increment: +{count_after - count_before:,}\")\n",
    "        \n",
    "        print(json.dumps(result))\n",
    "        return 0\n",
    "        \n",
    "    except Exception as e:\n",
    "        error = {\n",
    "            \"status\": \"error\",\n",
    "            \"error_message\": str(e),\n",
    "            \"error_type\": type(e).__name__,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        }\n",
    "        print(json.dumps(error))\n",
    "        return 1\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sys.exit(main())\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa7b7f9",
   "metadata": {},
   "source": [
    "##### Function to create python script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4de1fff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_n8n_executable_script(script_path, script_content):\n",
    "    \n",
    "    with open(script_path, 'w') as file:\n",
    "        file.write(script_content)\n",
    "    \n",
    "    # Make the script executable for n8n\n",
    "    os.chmod(script_path, 0o755)\n",
    "        \n",
    "    print(f\"Script created: {script_path}\")\n",
    "    print(\"Can execute by n8n with the node 'Execute Command'\")\n",
    "    \n",
    "    return script_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8302d11b",
   "metadata": {},
   "source": [
    "##### Execute function and extract path to probe locally after this execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e0e8fc22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script created: /home/jovyan/challenge/notebooks/n8n_ingest_script.py\n",
      "Can execute by n8n with the node 'Execute Command'\n",
      "Command to n8n: python3 /notebooks/n8n_ingest_script.py\n"
     ]
    }
   ],
   "source": [
    "# Create script\n",
    "script_path = \"/home/jovyan/challenge/notebooks/n8n_ingest_script.py\"\n",
    "script_path_ingest_data = create_n8n_executable_script(script_path, script_content_ingest_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb9bf61",
   "metadata": {},
   "source": [
    "##### Probe python script locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c29f0096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Probe script locally\n",
      "Return code: 0\n",
      "Output: duckdb already installed\n",
      "pandas already installed\n",
      "csv loaded: 2,000 rows, 10 columns\n",
      "load_date: 2025-08-28 17:17:17.018153\n",
      "source_file_name: ads_spend.csv\n",
      "Table ads_spend_db verified/created\n",
      "Register added: 2,000\n",
      "Total regiser in DB now: 26,000\n",
      "Increment: +2,000\n",
      "{\"status\": \"success\", \"timestamp\": \"2025-08-28T17:17:17.300740\", \"rows_inserted\": 2000, \"total_rows_before\": 24000, \"total_rows_after\": 26000, \"source_file\": \"ads_spend.csv\", \"message\": \"Successfully ingested 2,000 rows into warehouse\"}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Probar el script localmente\n",
    "print(f\"\\nProbe script locally\")\n",
    "import subprocess\n",
    "del conn\n",
    "result = subprocess.run(['python3', script_path_ingest_data], \n",
    "                       capture_output=True, text=True, cwd='/home/jovyan/challenge')\n",
    "\n",
    "print(f\"Return code: {result.returncode}\")\n",
    "print(f\"Output: {result.stdout}\")\n",
    "if result.stderr:\n",
    "    print(f\"Errors: {result.stderr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0da82e8",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73e28ca",
   "metadata": {},
   "source": [
    "### Data validation requeried for metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084882dd",
   "metadata": {},
   "source": [
    "##### Verify if we have enough daterange for metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b66d08b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to warehouse database\n",
      "\n",
      "Unique days in dataset: 181\n"
     ]
    }
   ],
   "source": [
    "#localprobe\n",
    "db_path = \"/home/jovyan/challenge/database/warehouse.db\"\n",
    "\n",
    "# Verify database exists\n",
    "if not os.path.exists(db_path):\n",
    "    raise FileNotFoundError(f\"Database not found: {db_path}\")\n",
    "\n",
    "# Connect to DuckDB\n",
    "conn = duckdb.connect(db_path)\n",
    "print(\"Connected to warehouse database\")\n",
    "\n",
    "# Check if we have enough data for 60 days analysis\n",
    "days_available = conn.execute(\"\"\"\n",
    "    SELECT COUNT(DISTINCT date) as unique_days \n",
    "    FROM ads_spend_db\n",
    "\"\"\").fetchone()[0]\n",
    "\n",
    "print(f\"\\nUnique days in dataset: {days_available}\")\n",
    "\n",
    "if days_available < 30:\n",
    "    print(\"Warning: Less than 30 days of data available\")\n",
    "elif days_available < 60:\n",
    "    print(\"Warning: Less than 60 days of data available for comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed99c787",
   "metadata": {},
   "source": [
    "##### Probe metrics queries and final table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8cfff0ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    metric last_30_days prior_30_days percent_change\n",
      "CAC (Cost per Acquisition)       $29.81        $32.27          -7.6%\n",
      " ROAS (Return on Ad Spend)         3.35           3.1           8.1%\n"
     ]
    }
   ],
   "source": [
    "kpi_query = \"\"\"\n",
    "WITH last_30_days AS (\n",
    "    SELECT \n",
    "        SUM(spend) as total_spend,\n",
    "        SUM(conversions) as total_conversions,\n",
    "        SUM(conversions * 100) as total_revenue,\n",
    "        COUNT(DISTINCT date) as days_count\n",
    "    FROM ads_spend_db \n",
    "    WHERE date >= (SELECT MAX(date) - INTERVAL 29 DAY FROM ads_spend_db)\n",
    "        AND date <= (SELECT MAX(date) FROM ads_spend_db)\n",
    "),\n",
    "\n",
    "prior_30_days AS (\n",
    "    SELECT \n",
    "        SUM(spend) as total_spend,\n",
    "        SUM(conversions) as total_conversions,\n",
    "        SUM(conversions * 100) as total_revenue,\n",
    "        COUNT(DISTINCT date) as days_count\n",
    "    FROM ads_spend_db \n",
    "    WHERE date >= (SELECT MAX(date) - INTERVAL 59 DAY FROM ads_spend_db)\n",
    "        AND date <= (SELECT MAX(date) - INTERVAL 30 DAY FROM ads_spend_db)\n",
    "),\n",
    "\n",
    "metrics_comparison AS (\n",
    "    SELECT \n",
    "        -- Last 30 Days Metrics\n",
    "        ROUND(l.total_spend / NULLIF(l.total_conversions, 0), 2) as cac_last_30,\n",
    "        ROUND(l.total_revenue / NULLIF(l.total_spend, 0), 2) as roas_last_30,\n",
    "        \n",
    "        -- Prior 30 Days Metrics  \n",
    "        ROUND(p.total_spend / NULLIF(p.total_conversions, 0), 2) as cac_prior_30,\n",
    "        ROUND(p.total_revenue / NULLIF(p.total_spend, 0), 2) as roas_prior_30,\n",
    "        \n",
    "        -- Raw values for context\n",
    "        l.total_spend as spend_last_30,\n",
    "        l.total_conversions as conversions_last_30,\n",
    "        l.days_count as days_last_30,\n",
    "        p.total_spend as spend_prior_30,\n",
    "        p.total_conversions as conversions_prior_30,\n",
    "        p.days_count as days_prior_30\n",
    "        \n",
    "    FROM last_30_days l\n",
    "    CROSS JOIN prior_30_days p\n",
    ")\n",
    "\n",
    "SELECT \n",
    "    'CAC (Cost per Acquisition)' as metric,\n",
    "    '$' || CAST(cac_last_30 AS VARCHAR) as last_30_days,\n",
    "    '$' || CAST(cac_prior_30 AS VARCHAR) as prior_30_days,\n",
    "    CASE \n",
    "        WHEN cac_prior_30 = 0 OR cac_prior_30 IS NULL THEN 'N/A'\n",
    "        ELSE CAST(ROUND(((cac_last_30 - cac_prior_30) / cac_prior_30) * 100, 1) AS VARCHAR) || '%'\n",
    "    END as percent_change\n",
    "FROM metrics_comparison\n",
    "\n",
    "UNION ALL\n",
    "\n",
    "SELECT \n",
    "    'ROAS (Return on Ad Spend)' as metric,\n",
    "    CAST(roas_last_30 AS VARCHAR) as last_30_days,\n",
    "    CAST(roas_prior_30 AS VARCHAR) as prior_30_days,\n",
    "    CASE \n",
    "        WHEN roas_prior_30 = 0 OR roas_prior_30 IS NULL THEN 'N/A'\n",
    "        ELSE CAST(ROUND(((roas_last_30 - roas_prior_30) / roas_prior_30) * 100, 1) AS VARCHAR) || '%'\n",
    "    END as percent_change\n",
    "FROM metrics_comparison;\n",
    "\"\"\"\n",
    "\n",
    "kpi_results = conn.execute(kpi_query).fetchdf()\n",
    "print(kpi_results.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2415c642",
   "metadata": {},
   "source": [
    "##### Function to get metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49f97e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_kpis():\n",
    "\n",
    "    # Error handling\n",
    "    try:\n",
    "\n",
    "        # Localprobe\n",
    "        db_path = \"/home/jovyan/challenge/database/warehouse.db\"\n",
    "        # Production\n",
    "        #db_path = \"/database/warehouse.db\"\n",
    "        \n",
    "        # Verify database exists\n",
    "        if not os.path.exists(db_path):\n",
    "            raise FileNotFoundError(f\"Database not found: {db_path}\")\n",
    "        \n",
    "        # Connect to DuckDB\n",
    "        conn = duckdb.connect(db_path)\n",
    "        print(\"Connected to warehouse database\")\n",
    "        \n",
    "        # Check if we have enough data for 60 days analysis\n",
    "        days_available = conn.execute(\"\"\"\n",
    "            SELECT COUNT(DISTINCT date) as unique_days \n",
    "            FROM ads_spend_db\n",
    "        \"\"\").fetchone()[0]\n",
    "        \n",
    "        print(f\"\\nUnique days in dataset: {days_available}\")\n",
    "        \n",
    "        if days_available < 30:\n",
    "            print(\"Warning: Less than 30 days of data available\")\n",
    "        elif days_available < 60:\n",
    "            print(\"Warning: Less than 60 days of data available for comparison\")\n",
    "\n",
    "        # Final kpi query        \n",
    "        kpi_query = \"\"\"\n",
    "        WITH last_30_days AS (\n",
    "            SELECT \n",
    "                SUM(spend) as total_spend,\n",
    "                SUM(conversions) as total_conversions,\n",
    "                SUM(conversions * 100) as total_revenue,\n",
    "                COUNT(DISTINCT date) as days_count\n",
    "            FROM ads_spend_db \n",
    "            WHERE date >= (SELECT MAX(date) - INTERVAL 29 DAY FROM ads_spend_db)\n",
    "              AND date <= (SELECT MAX(date) FROM ads_spend_db)\n",
    "        ),\n",
    "        \n",
    "        prior_30_days AS (\n",
    "            SELECT \n",
    "                SUM(spend) as total_spend,\n",
    "                SUM(conversions) as total_conversions,\n",
    "                SUM(conversions * 100) as total_revenue,\n",
    "                COUNT(DISTINCT date) as days_count\n",
    "            FROM ads_spend_db \n",
    "            WHERE date >= (SELECT MAX(date) - INTERVAL 59 DAY FROM ads_spend_db)\n",
    "              AND date <= (SELECT MAX(date) - INTERVAL 30 DAY FROM ads_spend_db)\n",
    "        ),\n",
    "        \n",
    "        metrics_comparison AS (\n",
    "            SELECT \n",
    "                -- Last 30 Days Metrics\n",
    "                ROUND(l.total_spend / NULLIF(l.total_conversions, 0), 2) as cac_last_30,\n",
    "                ROUND(l.total_revenue / NULLIF(l.total_spend, 0), 2) as roas_last_30,\n",
    "                \n",
    "                -- Prior 30 Days Metrics  \n",
    "                ROUND(p.total_spend / NULLIF(p.total_conversions, 0), 2) as cac_prior_30,\n",
    "                ROUND(p.total_revenue / NULLIF(p.total_spend, 0), 2) as roas_prior_30,\n",
    "                \n",
    "                -- Raw values for context\n",
    "                l.total_spend as spend_last_30,\n",
    "                l.total_conversions as conversions_last_30,\n",
    "                l.days_count as days_last_30,\n",
    "                p.total_spend as spend_prior_30,\n",
    "                p.total_conversions as conversions_prior_30,\n",
    "                p.days_count as days_prior_30\n",
    "                \n",
    "            FROM last_30_days l\n",
    "            CROSS JOIN prior_30_days p\n",
    "        )\n",
    "        \n",
    "        SELECT \n",
    "            'CAC (Cost per Acquisition)' as metric,\n",
    "            '$' || CAST(cac_last_30 AS VARCHAR) as last_30_days,\n",
    "            '$' || CAST(cac_prior_30 AS VARCHAR) as prior_30_days,\n",
    "            CASE \n",
    "                WHEN cac_prior_30 = 0 OR cac_prior_30 IS NULL THEN 'N/A'\n",
    "                ELSE CAST(ROUND(((cac_last_30 - cac_prior_30) / cac_prior_30) * 100, 1) AS VARCHAR) || '%'\n",
    "            END as percent_change\n",
    "        FROM metrics_comparison\n",
    "        \n",
    "        UNION ALL\n",
    "        \n",
    "        SELECT \n",
    "            'ROAS (Return on Ad Spend)' as metric,\n",
    "            CAST(roas_last_30 AS VARCHAR) as last_30_days,\n",
    "            CAST(roas_prior_30 AS VARCHAR) as prior_30_days,\n",
    "            CASE \n",
    "                WHEN roas_prior_30 = 0 OR roas_prior_30 IS NULL THEN 'N/A'\n",
    "                ELSE CAST(ROUND(((roas_last_30 - roas_prior_30) / roas_prior_30) * 100, 1) AS VARCHAR) || '%'\n",
    "            END as percent_change\n",
    "        FROM metrics_comparison;\n",
    "        \"\"\"\n",
    "        \n",
    "        # Get kpi table\n",
    "        kpi_results = conn.execute(kpi_query).fetchdf()\n",
    "        print(kpi_results.to_string(index=False))\n",
    "\n",
    "        # Get some database info\n",
    "        data_overview = conn.execute(\"\"\"\n",
    "            SELECT \n",
    "                MIN(date) as earliest_date,\n",
    "                MAX(date) as latest_date,\n",
    "                COUNT(*) as total_records,\n",
    "                COUNT(DISTINCT platform) as platforms,\n",
    "                SUM(spend) as total_spend,\n",
    "                SUM(conversions) as total_conversions\n",
    "            FROM ads_spend_db\n",
    "        \"\"\").fetchdf()\n",
    "\n",
    "        # Results in dictionary format\n",
    "        results_summary = {\n",
    "            \"analysis_date\": datetime.now().isoformat(),\n",
    "            \"data_range\": {\n",
    "                \"earliest_date\": str(data_overview['earliest_date'].iloc[0]),\n",
    "                \"latest_date\": str(data_overview['latest_date'].iloc[0]),\n",
    "                \"total_records\": int(data_overview['total_records'].iloc[0])\n",
    "            },\n",
    "            \"kpi_comparison\": kpi_results.to_dict('records')\n",
    "        }\n",
    "        print(json.dumps(results_summary))\n",
    "                \n",
    "        conn.close()\n",
    "        return\n",
    "        \n",
    "    except Exception as e:\n",
    "        error = {\n",
    "            \"status\": \"error\",\n",
    "            \"error_message\": str(e),\n",
    "            \"error_type\": type(e).__name__,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        }\n",
    "        print(json.dumps(error))\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e792b6",
   "metadata": {},
   "source": [
    "##### Execute metric function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b3027800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to warehouse database\n",
      "\n",
      "Unique days in dataset: 181\n",
      "                    metric last_30_days prior_30_days percent_change\n",
      "CAC (Cost per Acquisition)       $29.81        $32.27          -7.6%\n",
      " ROAS (Return on Ad Spend)         3.35           3.1           8.1%\n",
      "{\"analysis_date\": \"2025-08-28T17:17:40.210086\", \"data_range\": {\"earliest_date\": \"2025-01-01 00:00:00\", \"latest_date\": \"2025-06-30 00:00:00\", \"total_records\": 26000}, \"kpi_comparison\": [{\"metric\": \"CAC (Cost per Acquisition)\", \"last_30_days\": \"$29.81\", \"prior_30_days\": \"$32.27\", \"percent_change\": \"-7.6%\"}, {\"metric\": \"ROAS (Return on Ad Spend)\", \"last_30_days\": \"3.35\", \"prior_30_days\": \"3.1\", \"percent_change\": \"8.1%\"}]}\n"
     ]
    }
   ],
   "source": [
    "analyze_kpis()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348d780e",
   "metadata": {},
   "source": [
    "##### Create script for n8n in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e3ccff2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python script content\n",
    "script_content_metrics = '''#!/usr/bin/env python3\n",
    "\n",
    "import importlib\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# We list special packages that don't exist in jupyter installation\n",
    "special_required_packages = {\n",
    "    \"duckdb\": \"duckdb\",\n",
    "    \"pandas\": \"pandas\"\n",
    "}\n",
    "\n",
    "# Verify special packages\n",
    "for module_name, pip_name in special_required_packages.items():\n",
    "    try:\n",
    "        importlib.import_module(module_name)\n",
    "        #print(f\"{module_name} already installed\")\n",
    "    except ImportError:\n",
    "        #print(f\"Installing {pip_name}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pip_name])\n",
    "\n",
    "#import sys\n",
    "import pandas as pd\n",
    "import duckdb\n",
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "\n",
    "def main():\n",
    "        # Error handling\n",
    "    try:\n",
    "\n",
    "        # Localprobe\n",
    "        #db_path = \"/home/jovyan/challenge/database/warehouse.db\"\n",
    "        # Production\n",
    "        db_path = \"/database/warehouse.db\"\n",
    "        \n",
    "        # Verify database exists\n",
    "        if not os.path.exists(db_path):\n",
    "            raise FileNotFoundError(f\"Database not found: {db_path}\")\n",
    "        \n",
    "        # Connect to DuckDB\n",
    "        conn = duckdb.connect(db_path)\n",
    "        #print(\"Connected to warehouse database\")\n",
    "        \n",
    "        # Check if we have enough data for 60 days analysis\n",
    "        days_available = conn.execute(\"\"\"\n",
    "            SELECT COUNT(DISTINCT date) as unique_days \n",
    "            FROM ads_spend_db\n",
    "        \"\"\").fetchone()[0]\n",
    "        \n",
    "        print(f\"Unique days in dataset: {days_available}\")\n",
    "        \n",
    "        if days_available < 30:\n",
    "            print(\"Warning: Less than 30 days of data available\")\n",
    "        elif days_available < 60:\n",
    "            print(\"Warning: Less than 60 days of data available for comparison\")\n",
    "\n",
    "        # Final kpi query        \n",
    "        kpi_query = \"\"\"\n",
    "        WITH last_30_days AS (\n",
    "            SELECT \n",
    "                SUM(spend) as total_spend,\n",
    "                SUM(conversions) as total_conversions,\n",
    "                SUM(conversions * 100) as total_revenue,\n",
    "                COUNT(DISTINCT date) as days_count\n",
    "            FROM ads_spend_db \n",
    "            WHERE date >= (SELECT MAX(date) - INTERVAL 29 DAY FROM ads_spend_db)\n",
    "              AND date <= (SELECT MAX(date) FROM ads_spend_db)\n",
    "        ),\n",
    "        \n",
    "        prior_30_days AS (\n",
    "            SELECT \n",
    "                SUM(spend) as total_spend,\n",
    "                SUM(conversions) as total_conversions,\n",
    "                SUM(conversions * 100) as total_revenue,\n",
    "                COUNT(DISTINCT date) as days_count\n",
    "            FROM ads_spend_db \n",
    "            WHERE date >= (SELECT MAX(date) - INTERVAL 59 DAY FROM ads_spend_db)\n",
    "              AND date <= (SELECT MAX(date) - INTERVAL 30 DAY FROM ads_spend_db)\n",
    "        ),\n",
    "        \n",
    "        metrics_comparison AS (\n",
    "            SELECT \n",
    "                -- Last 30 Days Metrics\n",
    "                ROUND(l.total_spend / NULLIF(l.total_conversions, 0), 2) as cac_last_30,\n",
    "                ROUND(l.total_revenue / NULLIF(l.total_spend, 0), 2) as roas_last_30,\n",
    "                \n",
    "                -- Prior 30 Days Metrics  \n",
    "                ROUND(p.total_spend / NULLIF(p.total_conversions, 0), 2) as cac_prior_30,\n",
    "                ROUND(p.total_revenue / NULLIF(p.total_spend, 0), 2) as roas_prior_30,\n",
    "                \n",
    "                -- Raw values for context\n",
    "                l.total_spend as spend_last_30,\n",
    "                l.total_conversions as conversions_last_30,\n",
    "                l.days_count as days_last_30,\n",
    "                p.total_spend as spend_prior_30,\n",
    "                p.total_conversions as conversions_prior_30,\n",
    "                p.days_count as days_prior_30\n",
    "                \n",
    "            FROM last_30_days l\n",
    "            CROSS JOIN prior_30_days p\n",
    "        )\n",
    "        \n",
    "        SELECT \n",
    "            'CAC (Cost per Acquisition)' as metric,\n",
    "            '$' || CAST(cac_last_30 AS VARCHAR) as last_30_days,\n",
    "            '$' || CAST(cac_prior_30 AS VARCHAR) as prior_30_days,\n",
    "            CASE \n",
    "                WHEN cac_prior_30 = 0 OR cac_prior_30 IS NULL THEN 'N/A'\n",
    "                ELSE CAST(ROUND(((cac_last_30 - cac_prior_30) / cac_prior_30) * 100, 1) AS VARCHAR) || '%'\n",
    "            END as percent_change\n",
    "        FROM metrics_comparison\n",
    "        \n",
    "        UNION ALL\n",
    "        \n",
    "        SELECT \n",
    "            'ROAS (Return on Ad Spend)' as metric,\n",
    "            CAST(roas_last_30 AS VARCHAR) as last_30_days,\n",
    "            CAST(roas_prior_30 AS VARCHAR) as prior_30_days,\n",
    "            CASE \n",
    "                WHEN roas_prior_30 = 0 OR roas_prior_30 IS NULL THEN 'N/A'\n",
    "                ELSE CAST(ROUND(((roas_last_30 - roas_prior_30) / roas_prior_30) * 100, 1) AS VARCHAR) || '%'\n",
    "            END as percent_change\n",
    "        FROM metrics_comparison;\n",
    "        \"\"\"\n",
    "        \n",
    "        # Get kpi table\n",
    "        kpi_results = conn.execute(kpi_query).fetchdf()\n",
    "        #print(kpi_results.to_string(index=False))\n",
    "\n",
    "        # Get some database info\n",
    "        data_overview = conn.execute(\"\"\"\n",
    "            SELECT \n",
    "                MIN(date) as earliest_date,\n",
    "                MAX(date) as latest_date,\n",
    "                COUNT(*) as total_records,\n",
    "                COUNT(DISTINCT platform) as platforms,\n",
    "                SUM(spend) as total_spend,\n",
    "                SUM(conversions) as total_conversions\n",
    "            FROM ads_spend_db\n",
    "        \"\"\").fetchdf()\n",
    "\n",
    "        # Results in dictionary format\n",
    "        results_summary = {\n",
    "            \"analysis_date\": datetime.now().isoformat(),\n",
    "            \"data_range\": {\n",
    "                \"earliest_date\": str(data_overview['earliest_date'].iloc[0]),\n",
    "                \"latest_date\": str(data_overview['latest_date'].iloc[0]),\n",
    "                \"total_records\": int(data_overview['total_records'].iloc[0])\n",
    "            },\n",
    "            \"kpi_comparison\": kpi_results.to_dict('records')\n",
    "        }\n",
    "        print(json.dumps(results_summary))\n",
    "                \n",
    "        conn.close()\n",
    "        return 0\n",
    "        \n",
    "    except Exception as e:\n",
    "        error = {\n",
    "            \"status\": \"error\",\n",
    "            \"error_message\": str(e),\n",
    "            \"error_type\": type(e).__name__,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        }\n",
    "        print(json.dumps(error))\n",
    "        return 1\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sys.exit(main())\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398aea40",
   "metadata": {},
   "source": [
    "##### Execute function and extract path to probe locally after this execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ab891a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script created: /home/jovyan/challenge/notebooks/n8n_metrics_script.py\n",
      "Can execute by n8n with the node 'Execute Command'\n",
      "Command to n8n: python3 /notebooks/n8n_ingest_script.py\n"
     ]
    }
   ],
   "source": [
    "# Create script\n",
    "script_path = \"/home/jovyan/challenge/notebooks/n8n_metrics_script.py\"\n",
    "script_path_metrics = create_n8n_executable_script(script_path, script_content_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9cd4cc",
   "metadata": {},
   "source": [
    "##### Probe python script locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c30d1fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Probe script locally\n",
      "Return code: 0\n",
      "Output: duckdb already installed\n",
      "pandas already installed\n",
      "Connected to warehouse database\n",
      "Unique days in dataset: 181\n",
      "                    metric last_30_days prior_30_days percent_change\n",
      "CAC (Cost per Acquisition)       $29.81        $32.27          -7.6%\n",
      " ROAS (Return on Ad Spend)         3.35           3.1           8.1%\n",
      "{\"analysis_date\": \"2025-08-28T17:25:48.337979\", \"data_range\": {\"earliest_date\": \"2025-01-01 00:00:00\", \"latest_date\": \"2025-06-30 00:00:00\", \"total_records\": 26000}, \"kpi_comparison\": [{\"metric\": \"CAC (Cost per Acquisition)\", \"last_30_days\": \"$29.81\", \"prior_30_days\": \"$32.27\", \"percent_change\": \"-7.6%\"}, {\"metric\": \"ROAS (Return on Ad Spend)\", \"last_30_days\": \"3.35\", \"prior_30_days\": \"3.1\", \"percent_change\": \"8.1%\"}]}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Probar el script localmente\n",
    "print(f\"\\nProbe script locally\")\n",
    "import subprocess\n",
    "result = subprocess.run(['python3', script_path_metrics], \n",
    "                       capture_output=True, text=True, cwd='/home/jovyan/challenge')\n",
    "\n",
    "print(f\"Return code: {result.returncode}\")\n",
    "print(f\"Output: {result.stdout}\")\n",
    "if result.stderr:\n",
    "    print(f\"Errors: {result.stderr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0073d0",
   "metadata": {},
   "source": [
    "# Part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a954ec",
   "metadata": {},
   "source": [
    "### Get Data with API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "bb59b9c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status code: 200\n",
      "Data:\n",
      "\n",
      " {'analysis_date': '2025-08-29T19:50:54.614379', 'data_range': {'earliest_date': '2025-01-01 00:00:00', 'latest_date': '2025-06-30 00:00:00', 'total_records': 26000}, 'kpi_comparison': [{'metric': 'CAC (Cost per Acquisition)', 'last_30_days': '$29.81', 'prior_30_days': '$32.27', 'percent_change': '-7.6%'}, {'metric': 'ROAS (Return on Ad Spend)', 'last_30_days': '3.35', 'prior_30_days': '3.1', 'percent_change': '8.1%'}]}\n"
     ]
    }
   ],
   "source": [
    "url = \"http://n8n:5678/webhook-test/api?endpoint=metrics\"\n",
    "\n",
    "try:\n",
    "    # Call API\n",
    "    response = requests.get(url)\n",
    "\n",
    "    print(\"Status code:\", response.status_code)\n",
    "    print(\"Data:\\n\\n\", response.json())\n",
    "\n",
    "except requests.exceptions.RequestException as error:\n",
    "    print(\"Error:\", error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94636502",
   "metadata": {},
   "source": [
    "# Part 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483fb457",
   "metadata": {},
   "source": [
    "### Experiment with api"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f028adad",
   "metadata": {},
   "source": [
    "##### Verify response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9aa859b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status code: 200\n",
      "Error: Expecting value: line 1 column 1 (char 0)\n"
     ]
    }
   ],
   "source": [
    "url = \"http://n8n:5678/webhook-test/api\"\n",
    "params = {\n",
    "    \"question\": \"hola que tal\",\n",
    "    \"agent\": \"yes\"\n",
    "}\n",
    "\n",
    "try:\n",
    "    # Call API\n",
    "    response = requests.get(url, params=params)\n",
    "\n",
    "    print(\"Status code:\", response.status_code)\n",
    "    print(\"Data:\\n\\n\", response.json())\n",
    "\n",
    "except requests.exceptions.RequestException as error:\n",
    "    print(\"Error:\", error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408b5b4a",
   "metadata": {},
   "source": [
    "##### Integer NL simple example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3a2df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_response(data):\n",
    "    \n",
    "    # Transform to human lenguage\n",
    "    if \"error\" in data:\n",
    "        return f\"Error getting data: {data['error']}\"\n",
    "    \n",
    "    answer = \"Marketing Performance Analysis\\n\\n\"\n",
    "    \n",
    "    # Summary\n",
    "    if \"kpi_comparison\" in data:\n",
    "        s_cac = data.get(\"kpi_comparison\", [{}])[0]\n",
    "        s_roas = data.get(\"kpi_comparison\", [{}])[0]\n",
    "        answer += \"**Key Metrics:**\\n\"\n",
    "        answer += f\"• last_30_days CAC: ${s_cac.get('last_30_days', 'N/A')}\\n\"\n",
    "        answer += f\"• last_30_days ROAS: {s_roas.get('last_30_days', 'N/A')}x\\n\"\n",
    "        answer += f\"• prior_30_days CAC: ${s_cac.get('prior_30_days', 'N/A')}\\n\"\n",
    "        answer += f\"• prior_30_days ROAS: {s_roas.get('prior_30_days', 'N/A')}x\\n\"\n",
    "        \n",
    "        # Percent Change\n",
    "        cac_change = s_cac.get('percent_change', 'N/A')\n",
    "        roas_change = s_roas.get('percent_change', 'N/A')\n",
    "\n",
    "        # Try to parse to float if it's a string with %\n",
    "        def parse_change(val):\n",
    "            if val is None:\n",
    "                return None\n",
    "            if isinstance(val, (int, float)):\n",
    "                return val\n",
    "            try:\n",
    "                return float(str(val).replace(\"%\", \"\").strip())\n",
    "            except ValueError:\n",
    "                return None\n",
    "\n",
    "        cac_change_val = parse_change(cac_change)\n",
    "        roas_change_val = parse_change(roas_change)\n",
    "\n",
    "        \n",
    "        if cac_change_val is not None:\n",
    "            trend = \"📉\" if cac_change_val < 0 else \"📈\"\n",
    "            answer += f\"• CAC Change: {trend} {cac_change}%\\n\"\n",
    "            \n",
    "        if roas_change_val is not None:\n",
    "            trend = \"📈\" if roas_change_val > 0 else \"📉\"  \n",
    "            answer += f\"• ROAS Change: {trend} {roas_change}%\\n\"\n",
    "    \n",
    "    answer += \"\\n **Lower CAC + Higher ROAS = Better Performance!**\"\n",
    "    \n",
    "    return answer\n",
    "\n",
    "def main():\n",
    "    \n",
    "    # Default question if the petition doesn't have argument\n",
    "    question = sys.argv[1] if len(sys.argv) > 1 else \"percent change\"\n",
    "    params = {\n",
    "        \"question\": \"hola que tal\",\n",
    "        \"agent\": \"yes\"\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Call Data\n",
    "        response = requests.get(\"http://n8n:5678/webhook-test/api\", params=params)\n",
    "        api_data = response.json()\n",
    "        \n",
    "        # Format human response\n",
    "        answer = format_response(api_data)\n",
    "        \n",
    "        # Return JSON for webhook\n",
    "        result = {\n",
    "            \"success\": True,\n",
    "            \"question\": question,\n",
    "            \"answer\": answer,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        print(json.dumps(result))\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Error response\n",
    "        error_result = {\n",
    "            \"success\": False,\n",
    "            \"question\": question,\n",
    "            \"error\": str(e),\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        print(json.dumps(error_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3bac55c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"success\": true, \"question\": \"-f\", \"answer\": \"Marketing Performance Analysis\\n\\n**Key Metrics:**\\n\\u2022 last_30_days CAC: $$29.81\\n\\u2022 last_30_days ROAS: $29.81x\\n\\u2022 prior_30_days CAC: $$32.27\\n\\u2022 prior_30_days ROAS: $32.27x\\n\\u2022 CAC Change: \\ud83d\\udcc9 -7.6%%\\n\\u2022 ROAS Change: \\ud83d\\udcc9 -7.6%%\\n\\n **Lower CAC + Higher ROAS = Better Performance!**\", \"timestamp\": \"2025-08-29T18:56:28.955134\"}\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9068c38",
   "metadata": {},
   "source": [
    "##### Create script for n8n in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590e1ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python script content\n",
    "script_content_agent = '''#!/usr/bin/env python3\n",
    "\n",
    "import json\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "def main():\n",
    "    \n",
    "    # Default question if the petition doesn't have argument\n",
    "    question = sys.argv[2] if len(sys.argv) > 1 else \"percent change\"\n",
    "    response = sys.argv[1]\n",
    "        \n",
    "    try:\n",
    "        # Call Data\n",
    "        #response = requests.get(\"http://n8n:5678/webhook-test/api\")\n",
    "        data = json.loads(response)\n",
    "        \n",
    "        # Transform to human lenguage\n",
    "        if \"error\" in data:\n",
    "            return f\"Error getting data: {data['error']}\"\n",
    "        \n",
    "        answer = \"Marketing Performance Analysis\"\n",
    "        \n",
    "        # Summary\n",
    "        if \"kpi_comparison\" in data:\n",
    "            s_cac = data.get(\"kpi_comparison\", [{}])[0]\n",
    "            s_roas = data.get(\"kpi_comparison\", [{}])[1]\n",
    "            answer += \"**Key Metrics:**\"\n",
    "            answer += f\"• last_30_days CAC: ${s_cac.get('last_30_days', 'N/A')}\"\n",
    "            answer += f\"• last_30_days ROAS: {s_roas.get('last_30_days', 'N/A')}x\"\n",
    "            answer += f\"• prior_30_days CAC: ${s_cac.get('prior_30_days', 'N/A')}\"\n",
    "            answer += f\"• prior_30_days ROAS: {s_roas.get('prior_30_days', 'N/A')}x\"\n",
    "            \n",
    "            # Percent Change\n",
    "            cac_change = s_cac.get('percent_change', 'N/A')\n",
    "            roas_change = s_roas.get('percent_change', 'N/A')\n",
    "\n",
    "            # Try to parse to float if it's a string with %\n",
    "            def parse_change(val):\n",
    "                if val is None:\n",
    "                    return None\n",
    "                if isinstance(val, (int, float)):\n",
    "                    return val\n",
    "                try:\n",
    "                    return float(str(val).replace(\"%\", \"\").strip())\n",
    "                except ValueError:\n",
    "                    return None\n",
    "\n",
    "            cac_change_val = parse_change(cac_change)\n",
    "            roas_change_val = parse_change(roas_change)\n",
    "\n",
    "            \n",
    "            if cac_change_val is not None:\n",
    "                trend = \"📉\" if cac_change_val < 0 else \"📈\"\n",
    "                answer += f\"• CAC Change: {trend} {cac_change}%\"\n",
    "                \n",
    "            if roas_change_val is not None:\n",
    "                trend = \"📈\" if roas_change_val > 0 else \"📉\"  \n",
    "                answer += f\"• ROAS Change: {trend} {roas_change}%\"\n",
    "        \n",
    "        answer += \"**Lower CAC + Higher ROAS = Better Performance!**\"\n",
    "        \n",
    "        # Return JSON for webhook\n",
    "        result = {\n",
    "            \"success\": True,\n",
    "            \"question\": question,\n",
    "            \"answer\": answer,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        print(json.dumps(result))\n",
    "        return 0\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Error response\n",
    "        error_result = {\n",
    "            \"success\": False,\n",
    "            \"question\": question,\n",
    "            \"error\": str(e),\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        print(json.dumps(error_result))\n",
    "        return 1\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sys.exit(main())\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e88d01a",
   "metadata": {},
   "source": [
    "##### Execute function and extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "6d87b30e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script created: /home/jovyan/challenge/notebooks/n8n_agent_script.py\n",
      "Can execute by n8n with the node 'Execute Command'\n"
     ]
    }
   ],
   "source": [
    "# Create script\n",
    "script_path = \"/home/jovyan/challenge/notebooks/n8n_agent_script.py\"\n",
    "script_path_ingest_data = create_n8n_executable_script(script_path, script_content_agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12679748",
   "metadata": {},
   "source": [
    "##### Final test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "63130969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data:\n",
      "\n",
      " Marketing Performance Analysis**Key Metrics:**• last_30_days CAC: $$29.81• last_30_days ROAS: 3.35x• prior_30_days CAC: $$32.27• prior_30_days ROAS: 3.1x• CAC Change: 📉 -7.6%%• ROAS Change: 📈 8.1%%**Lower CAC + Higher ROAS = Better Performance!**\n"
     ]
    }
   ],
   "source": [
    "url = \"http://n8n:5678/webhook-test/api\"\n",
    "params = {\n",
    "    \"question\": \"percentage change\",\n",
    "    \"agent\": \"yes\"\n",
    "}\n",
    "\n",
    "try:\n",
    "    # Call API\n",
    "    response = requests.get(url, params=params)\n",
    "    response = response.json()\n",
    "\n",
    "    print(\"Data:\\n\\n\", response[\"answer\"])\n",
    "\n",
    "except requests.exceptions.RequestException as error:\n",
    "    print(\"Error:\", error)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
